"""
    Chapter6 51.    それぞれの実験の結果をレポートにて報告せよ．
                    コードを変えることでどのような結果の変化が起こったか，またそれはなぜだと考えられるか考察せよ．
                    （例）多項式回帰にしてみる, 標準化せずにやってみる, bias=Falseにしてみる
                        epoch数, batch_size, loss関数, optimizer（の種類, またはパラメータ）を変更してみる
                        chapter3で紹介した分類問題をpytorchで解いてみる
                        など
"""

＜実行結果＞
(.venv) (base) Mac:Chapter6 s.takamiya$ python ex51-2.py
2.9.1
   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  ...  density    pH  sulphates  alcohol  quality
0            7.4              0.70         0.00             1.9      0.076  ...   0.9978  3.51       0.56      9.4        5
1            7.8              0.88         0.00             2.6      0.098  ...   0.9968  3.20       0.68      9.8        5
2            7.8              0.76         0.04             2.3      0.092  ...   0.9970  3.26       0.65      9.8        5
3           11.2              0.28         0.56             1.9      0.075  ...   0.9980  3.16       0.58      9.8        6
4            7.4              0.70         0.00             1.9      0.076  ...   0.9978  3.51       0.56      9.4        5

[5 rows x 12 columns]
epoch: 1000/1000, 
train_loss: 0.021565767005085945
test_loss:  0.018906068
係数:  -0.709421
bias:  0.66472644


＜実験結果＞

【実験】標準化無しの場合
epoch: 1000/1000, 
train_loss: 1.208327054977417
test_loss:  1.0411506
係数:  4.680103
bias:  5.794859

結果：精度悪化．スケールの違いにより学習が難航．
     Lossが 1.20 と大きいのは，入力と出力のスケールやレンジが違いすぎるため，勾配降下法がうまく収束していない．


【実験】bias = False　と変更し，標準化有りの場合
epoch: 1000/1000, 
train_loss: 0.049226779490709305
test_loss:  0.042779565
係数:  0.55500615
bias:  0

結果：係数が正負逆転．精度も基準の約2倍悪化．


【実験】bias = False　と変更し，標準化無しの場合
epoch: 1000/1000, 
train_loss: 1.2205826044082642
test_loss:  1.0523902
係数:  10.4934025
bias:  0

結果：精度悪化．


上記の実験から，標準化の有無，バイアスの有無による違いを考察する．
最も注目すべきは，標準化＋バイアスなしのケースである．
本来，ワインの密度（Density）とアルコール度数（Alcohol）には「負の相関（密度が高いほどアルコール分は低い）」があるため，基準モデルでは係数が -0.709 となっている．
しかし，バイアスをなくした途端，係数が 0.555（正の値） になってしまった．なぜこのような誤った学習が起きたのだろうか．
今回のプログラムで行っている標準化は (x - min) / (max - min) という Min-Max Scaling である．
これにより，データ x と t はすべて 0 以上 1 以下に変換される．
グラフで言うと，第1象限にデータが集まる．
原点 (0,0) からスタートして，第1象限にあるデータの塊の中を通ろうとすると，直線の傾き w は必ず正にならざるを得ない．
もし本来の負の傾きを表現しようとすれば，原点を通る直線 y = -ax は第4象限に向かってしまい，データ群から大きく外れてしまう．
その結果，モデルは相関関係（右肩下がり）を捉えることを諦め，せめてデータの真ん中を通ることを優先した結果，無理やり右肩上がりのプラスの係数を導き出してしまったと考えられる．